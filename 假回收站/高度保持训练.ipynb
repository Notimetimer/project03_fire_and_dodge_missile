{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16ea03cf",
   "metadata": {},
   "source": [
    "最简化仿真代码（非训练）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1667117",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from math import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 获取project目录\n",
    "def get_current_file_dir():\n",
    "    # 判断是否在 Jupyter Notebook 环境\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__  # ← 误报，不用管\n",
    "        if shell == 'ZMQInteractiveShell':  # Jupyter Notebook 或 JupyterLab\n",
    "            # 推荐用 os.getcwd()，指向启动 Jupyter 的目录\n",
    "            return os.getcwd()\n",
    "        else:  # 其他 shell\n",
    "            return os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # 普通 Python 脚本\n",
    "        return os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "current_dir = get_current_file_dir()\n",
    "sys.path.append(os.path.dirname(current_dir))\n",
    "from Envs.UAVmodel6d import UAVModel\n",
    "from Math_calculates.CartesianOnEarth import NUE2LLH, LLH2NUE\n",
    "\n",
    "# class height_track\n",
    "\n",
    "o00 = np.array([118, 30])  # 地理原点的经纬\n",
    "DEFAULT_RED_BIRTH_STATE = {'position': np.array([-38000.0, 8000.0, 0.0]),\n",
    "                               'psi': 0\n",
    "                               }\n",
    "dt_move=0.02\n",
    "UAV = UAVModel(dt=dt_move)\n",
    "UAV.ammo = 0\n",
    "UAV.id = 1\n",
    "UAV.red = True\n",
    "UAV.blue = False\n",
    "UAV.side = \"red\"\n",
    "UAV.color = np.array([1, 0, 0])\n",
    "# 红方出生点\n",
    "UAV.pos_ = DEFAULT_RED_BIRTH_STATE['position']\n",
    "UAV.speed = 300  # (UAV.speed_max - UAV.speed_min) / 2\n",
    "speed = UAV.speed\n",
    "UAV.psi = DEFAULT_RED_BIRTH_STATE['psi']\n",
    "UAV.theta = 0 * pi / 180\n",
    "UAV.gamma = 0 * pi / 180\n",
    "UAV.vel_ = UAV.speed * np.array([cos(UAV.theta) * cos(UAV.psi),\n",
    "                                    sin(UAV.theta),\n",
    "                                    cos(UAV.theta) * sin(UAV.psi)])\n",
    "lon_uav, lat_uav, h_uav = NUE2LLH(UAV.pos_[0], UAV.pos_[1], UAV.pos_[2], lon_o=o00[0], lat_o=o00[1], h_o=0)\n",
    "UAV.reset(lon0=lon_uav, lat0=lat_uav, h0=h_uav, v0=UAV.speed, psi0=UAV.psi, phi0=UAV.gamma,\n",
    "            theta0=UAV.theta, o00=o00)\n",
    "\n",
    "action = [0,0,0]\n",
    "\n",
    "target_height = 3000 + (action[0] + 1) / 2 * (10000 - 3000)  # 高度使用绝对数值\n",
    "delta_heading = action[1]  # 相对方位(弧度)\n",
    "target_speed = 170 + (action[2] + 1) / 2 * (544 - 170)  # 速度使用绝对数值\n",
    "# print('target_height',target_height)\n",
    "# for i in range(int(self.dt // dt_move)):\n",
    "t_last = 60\n",
    "\n",
    "tacview_show = 1\n",
    "\n",
    "if tacview_show:\n",
    "    from Visualize.tacview_visualize import *\n",
    "    tacview = Tacview()\n",
    "\n",
    "for i in range(int(t_last//dt_move)):\n",
    "    current_t = i*dt_move\n",
    "    UAV.move(target_height, delta_heading, target_speed)\n",
    "    loc_r = [UAV.lon, UAV.lat, UAV.alt]\n",
    "    if tacview_show:\n",
    "        data_to_send = ''\n",
    "        data_to_send += \"#%.2f\\n%s,T=%.6f|%.6f|%.6f|%.6f|%.6f|%.6f,Name=F16,Color=Red\\n\" % (\n",
    "                float(current_t), UAV.id, loc_r[0], loc_r[1], loc_r[2], UAV.phi * 180 / pi, UAV.theta * 180 / pi,\n",
    "                UAV.psi * 180 / pi)\n",
    "    if tacview_show:\n",
    "            tacview.send_data_to_client(data_to_send)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba6adf",
   "metadata": {},
   "source": [
    "高度追踪训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d29bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import torch as th\n",
    "from math import *\n",
    "from gym import spaces\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# 设置字体以支持中文\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 获取project目录\n",
    "def get_current_file_dir():\n",
    "    # 判断是否在 Jupyter Notebook 环境\n",
    "    try:\n",
    "        shell = get_ipython().__class__.__name__  # ← 误报，不用管\n",
    "        if shell == 'ZMQInteractiveShell':  # Jupyter Notebook 或 JupyterLab\n",
    "            # 推荐用 os.getcwd()，指向启动 Jupyter 的目录\n",
    "            return os.getcwd()\n",
    "        else:  # 其他 shell\n",
    "            return os.path.dirname(os.path.abspath(__file__))\n",
    "    except NameError:\n",
    "        # 普通 Python 脚本\n",
    "        return os.path.dirname(os.path.abspath(__file__))\n",
    "current_dir = get_current_file_dir()\n",
    "sys.path.append(os.path.dirname(current_dir))\n",
    "\n",
    "from Envs.UAVmodel6d import UAVModel\n",
    "from Math_calculates.CartesianOnEarth import NUE2LLH, LLH2NUE\n",
    "from Visualize.tacview_visualize import *\n",
    "from Visualize.tensorboard_visualize import *\n",
    "from Algorithms.SquashedPPOcontinues import *\n",
    "\n",
    "# # 只能处理不分块的多层全连接神经网络，没必要单独拿出来\n",
    "# def parse_model_sizes_from_meta(meta_path):\n",
    "#     \"\"\"\n",
    "#     从 meta json 中根据 'weight' 项推断网络层大小序列：\n",
    "#     返回 (state_dim, hidden_dim_list, action_dim)\n",
    "#     \"\"\"\n",
    "#     with open(meta_path, \"r\") as f:\n",
    "#         meta = json.load(f)\n",
    "#     weight_items = [(k, tuple(v)) for k, v in meta.items() if \"weight\" in k and len(v) == 2]\n",
    "#     weight_items.sort(key=lambda x: x[0])\n",
    "#     if not weight_items:\n",
    "#         raise RuntimeError(\"No weight items found in meta\")\n",
    "#     sizes = []\n",
    "#     sizes.append(weight_items[0][1][1])  # 第一个 weight 的 in_features -> input\n",
    "#     for _, shape in weight_items:\n",
    "#         sizes.append(shape[0])  # out_features\n",
    "#     state_dim_meta = sizes[0]\n",
    "#     if len(sizes) >= 3:\n",
    "#         hidden_meta = sizes[1:-1]\n",
    "#     else:\n",
    "#         hidden_meta = [sizes[1]] if len(sizes) == 2 else []\n",
    "#     action_dim_meta = sizes[-1]\n",
    "#     return state_dim_meta, hidden_meta, action_dim_meta\n",
    "\n",
    "# def build_agent_from_actor_meta(meta_path, device):\n",
    "#     state_dim_m, hidden_m, action_dim_m = parse_model_sizes_from_meta(meta_path)\n",
    "#     new_agent = PPOContinuous(state_dim_m, hidden_m, action_dim_m,\n",
    "#                               actor_lr, critic_lr, lmbda, epochs, eps, gamma, device)\n",
    "#     return new_agent\n",
    "\n",
    "class height_track_env():\n",
    "    def __init__(self, dt_move=0.02):\n",
    "        super(height_track_env, self).__init__()\n",
    "        self.UAV_ids = None\n",
    "        self.dt_report = None\n",
    "        self.dt_move = dt_move\n",
    "        self.t = None\n",
    "        # self.done = None\n",
    "        self.success = None # 胜\n",
    "        self.fail = None # 负\n",
    "        self.draw = None # 平\n",
    "        self.action_space = [spaces.Box(low=-1, high=+1, shape=(3,), dtype=np.float32)]\n",
    "        self.DEFAULT_RED_BIRTH_STATE = {'position': np.array([-38000.0, 8000.0, 0.0]),\n",
    "                               'psi': 0\n",
    "                               }\n",
    "        \n",
    "        # 高于升限会导致动作无法实施，影响\n",
    "        self.time_limit = 180 # 300 t_last\n",
    "        self.min_alt = 1e3\n",
    "        self.min_alt_save = 3e3\n",
    "        self.max_alt_save = 14e3\n",
    "        self.max_alt = 15e3\n",
    "\n",
    "        # △h动作输出有效性测试\n",
    "        self.height_req = None\n",
    "        \n",
    "        self.tacview_show = None\n",
    "    \n",
    "    def reset(self, o00=None, birth_state=None, height_req=8e3, dt_report=0.2, t0=0, tacview_show=0):\n",
    "        self.tacview_show = tacview_show\n",
    "        self.t = t0\n",
    "        self.success = 0\n",
    "        # self.done = 0\n",
    "        self.fail = 0\n",
    "        self.draw = 0\n",
    "        if o00 == None:\n",
    "            o00 = np.array([118, 30])  # 地理原点的经纬\n",
    "        if birth_state == None:\n",
    "            birth_state = self.DEFAULT_RED_BIRTH_STATE\n",
    "        self.dt_report = dt_report\n",
    "        UAV = UAVModel(dt=self.dt_move)\n",
    "        UAV.ammo = 0\n",
    "        UAV.id = 1\n",
    "        UAV.red = True\n",
    "        UAV.blue = False\n",
    "        UAV.side = \"red\"\n",
    "        UAV.color = np.array([1, 0, 0])\n",
    "        # 红方出生点\n",
    "        UAV.pos_ = self.DEFAULT_RED_BIRTH_STATE['position']\n",
    "        UAV.speed = 300  # (UAV.speed_max - UAV.speed_min) / 2\n",
    "        speed = UAV.speed\n",
    "        UAV.psi = self.DEFAULT_RED_BIRTH_STATE['psi']\n",
    "        UAV.theta = 0 * pi / 180\n",
    "        UAV.gamma = 0 * pi / 180\n",
    "        UAV.vel_ = UAV.speed * np.array([cos(UAV.theta) * cos(UAV.psi),\n",
    "                                            sin(UAV.theta),\n",
    "                                            cos(UAV.theta) * sin(UAV.psi)])\n",
    "        lon_uav, lat_uav, h_uav = NUE2LLH(UAV.pos_[0], UAV.pos_[1], UAV.pos_[2], lon_o=o00[0], lat_o=o00[1], h_o=0)\n",
    "        UAV.reset(lon0=lon_uav, lat0=lat_uav, h0=h_uav, v0=UAV.speed, psi0=UAV.psi, phi0=UAV.gamma,\n",
    "                    theta0=UAV.theta, o00=o00)\n",
    "        self.UAV = UAV\n",
    "        \n",
    "        # △h动作输出有效性测试\n",
    "        self.height_req = height_req\n",
    "\n",
    "        if tacview_show:\n",
    "            self.tacview = Tacview()\n",
    "\n",
    "\n",
    "    def get_obs(self):\n",
    "        '''\n",
    "        △h_abs / 5e3 m\n",
    "        h abs /5e3 m\n",
    "        h_dot /340 m/s\n",
    "        sin θ_v\n",
    "        cos θ_v\n",
    "        sin φ\n",
    "        cos φ\n",
    "        v /340 m/s\n",
    "        '''\n",
    "        v_hor = abs(self.UAV.vel_[0]**2+self.UAV.vel_[2]**2)\n",
    "        theta_v = atan2(self.UAV.vel_[1], v_hor)\n",
    "        obs = [\n",
    "            (self.height_req - self.UAV.alt) / 5e3,\n",
    "            self.UAV.alt / 5e3,\n",
    "            self.UAV.climb_rate /340,\n",
    "            sin(theta_v),\n",
    "            cos(theta_v),\n",
    "            sin(self.UAV.phi),\n",
    "            cos(self.UAV.phi),\n",
    "            self.UAV.speed /340,\n",
    "        ]\n",
    "        obs= np.array(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def get_obs_spaces(self):\n",
    "        self.reset()\n",
    "        obs = self.get_obs()\n",
    "        self.obs_spaces = [spaces.Box(low=-np.inf, high=+np.inf, shape=obs1.shape, dtype=np.float32) for obs1 in obs]\n",
    "        return self.obs_spaces\n",
    "\n",
    "    def step(self, action):\n",
    "        self.action = action\n",
    "        target_height, delta_heading, target_speed = action\n",
    "        self.t += self.dt_report\n",
    "        time_rate = int(round(self.dt_report/self.dt_move))\n",
    "        for _ in range(time_rate):\n",
    "            self.UAV.move(target_height, delta_heading, target_speed, relevant_height=True, relevant_speed=False, with_theta_req=False)\n",
    "            done = self.get_done()\n",
    "            # 单智能体特例\n",
    "            if self.fail:\n",
    "                break\n",
    "        next_obs = self.get_obs()\n",
    "        # done = self.get_done()\n",
    "        reward = self.get_reward()\n",
    "        \n",
    "        return next_obs, reward, done\n",
    "\n",
    "\n",
    "    def get_done(self):\n",
    "        done = 0\n",
    "        # 高度追踪失败条件：跑出h_min~h_max的范围立即失败\n",
    "        h_current = self.UAV.alt\n",
    "        if h_current<self.min_alt or h_current>self.max_alt:\n",
    "            done = 1\n",
    "            self.fail = 1\n",
    "            return done\n",
    "        \n",
    "        # 高度保持成功条件：到时间结束为止没有超出距离\n",
    "        if self.t>=self.time_limit:\n",
    "            done = 1\n",
    "            return done\n",
    "        \n",
    "        # 立即成功条件(暂时不做）：距离h_req小于100m，且爬升率绝对值小于10m/s\n",
    "        pass\n",
    "        return done\n",
    "\n",
    "\n",
    "    def get_reward(self, ):\n",
    "        # 高度奖励\n",
    "        h_current = self.UAV.alt\n",
    "        h_req = self.height_req\n",
    "        r_h_norm = (h_current<=h_req)*(h_current-self.min_alt)/(h_req-self.min_alt)+\\\n",
    "                    (h_current>h_req)*(1-(h_current-h_req)/(self.max_alt-h_req))\n",
    "        r_h_norm = 1 * r_h_norm\n",
    "\n",
    "        # 操作直接奖励\n",
    "        delta_height, delta_heading, target_speed = self.action\n",
    "        r_delta_height_instruction = 1-abs(h_req-(h_current+delta_height))/5000\n",
    "        r_h_norm += 0.8 * r_delta_height_instruction\n",
    "        \n",
    "\n",
    "        # 高度出界惩罚\n",
    "        if self.fail:\n",
    "            r_h_norm -= 10\n",
    "        if self.success:\n",
    "            r_h_norm += 3\n",
    "        \n",
    "        # 其他奖励待续\n",
    "        return r_h_norm\n",
    "        \n",
    "\n",
    "    def reder(self,):\n",
    "        loc_r = [self.UAV.lon, self.UAV.lat, self.UAV.alt]\n",
    "        if self.tacview_show:\n",
    "            data_to_send = ''\n",
    "            data_to_send += \"#%.2f\\n%s,T=%.6f|%.6f|%.6f|%.6f|%.6f|%.6f,Name=F16,Color=Red\\n\" % (\n",
    "                    float(self.t), self.UAV.id, loc_r[0], loc_r[1], loc_r[2], self.UAV.phi * 180 / pi, self.UAV.theta * 180 / pi,\n",
    "                    self.UAV.psi * 180 / pi)\n",
    "            self.tacview.send_data_to_client(data_to_send)\n",
    "\n",
    "\n",
    "env = height_track_env()\n",
    "from tqdm import tqdm\n",
    "obs_space = env.get_obs_spaces()\n",
    "action_space = env.action_space\n",
    "\n",
    "# dof = 3\n",
    "# 超参数\n",
    "actor_lr = 1e-4 # 1e-4 1e-6  # 2e-5 警告，学习率过大会出现\"nan\"\n",
    "critic_lr = actor_lr * 5  # *10 为什么critic学习率大于一都不会梯度爆炸？ 为什么设置成1e-5 也会爆炸？ chatgpt说要actor的2~10倍\n",
    "num_episodes = 400  # 2000 400\n",
    "hidden_dim = [128, 128]  # 128\n",
    "gamma = 0.9\n",
    "lmbda = 0.9\n",
    "epochs = 10  # 10\n",
    "eps = 0.2\n",
    "dt_decide = 2 # 2\n",
    "pre_train_rate = 0 # 0.25 # 0.25\n",
    "\n",
    "state_dim = len(obs_space) # obs_space[0].shape[0]  # env.observation_space.shape[0] # test\n",
    "action_dim = 1 # test\n",
    "action_bound = np.array([[-1,1]]*action_dim)  # 动作幅度限制, 必须使用双方括号，否则不能将不同维度分离\n",
    "env_name = '高低测试'\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "agent = PPOContinuous(state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                      lmbda, epochs, eps, gamma, device)\n",
    "\n",
    "# --- 仅保存一次网络形状（meta json），如果已存在则跳过\n",
    "# log_dir = \"./logs\"\n",
    "from datetime import datetime\n",
    "log_dir = os.path.join(\"./logs\", \"run-\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "actor_meta_path = os.path.join(log_dir, \"actor.meta.json\")\n",
    "critic_meta_path = os.path.join(log_dir, \"critic.meta.json\")\n",
    "\n",
    "def save_meta_once(path, state_dict):\n",
    "    if os.path.exists(path):\n",
    "        return\n",
    "    meta = {k: list(v.shape) for k, v in state_dict.items()}\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "\n",
    "save_meta_once(actor_meta_path, agent.actor.state_dict())\n",
    "save_meta_once(critic_meta_path, agent.critic.state_dict())\n",
    "\n",
    "from Math_calculates.ScaleLearningRate import scale_learning_rate\n",
    "# 根据动作维度缩放学习率\n",
    "actor_lr = scale_learning_rate(actor_lr, agent.actor)\n",
    "critic_lr = scale_learning_rate(critic_lr, agent.critic)\n",
    "\n",
    "from Visualize.tensorboard_visualize import TensorBoardLogger\n",
    "\n",
    "out_range_count = 0\n",
    "return_list = []\n",
    "steps_count = 0\n",
    "\n",
    "logger = TensorBoardLogger(log_root=log_dir, host=\"127.0.0.1\", port=6006, use_log_root=True)\n",
    "try:\n",
    "    # 有监督预训练\n",
    "    with tqdm(total=int(num_episodes*pre_train_rate), desc='Iteration') as pbar:  # 进度条\n",
    "        for i_episode in range(int(num_episodes*pre_train_rate)):  \n",
    "            episode_return = 0\n",
    "            transition_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': [], 'action_bounds': []}\n",
    "            \n",
    "            init_height = np.random.uniform(4000, 10000)  # 生成一个介于 4000 和 10000 的均匀分布值\n",
    "\n",
    "            birth_state={'position': np.array([-38000.0, init_height, 0.0]),\n",
    "                                'psi': 0\n",
    "                                }\n",
    "            height_req = np.random.uniform(4000, 10000)\n",
    "            env.reset(birth_state=birth_state, height_req=height_req, tacview_show=0, dt_report=dt_decide) # 打乱顺序也行啊\n",
    "            state = env.get_obs()\n",
    "            done = False\n",
    "            while not done:  # 每个训练回合\n",
    "                # 执行示范动作收集数据\n",
    "                action_height = np.clip(state[0],-1,1)\n",
    "                action=np.array([action_height], dtype=np.float32)\n",
    "\n",
    "                total_action = np.array([5000 * action[0], 0, 300]) # 1000 * \n",
    "\n",
    "                next_state, reward, done = env.step(total_action)\n",
    "\n",
    "                transition_dict['states'].append(state)\n",
    "                transition_dict['actions'].append(action)\n",
    "                transition_dict['next_states'].append(next_state)\n",
    "                transition_dict['rewards'].append(reward)\n",
    "                transition_dict['dones'].append(done)\n",
    "                transition_dict['action_bounds'].append(action_bound)\n",
    "                state = next_state\n",
    "                episode_return += reward * env.dt_report # 奖励按秒分析\n",
    "                steps_count += 1 # todo 增加一个以step为横轴的训练曲线\n",
    "\n",
    "            if env.fail==1:\n",
    "                out_range_count+=1\n",
    "\n",
    "            agent.update_actor_supervised(transition_dict)\n",
    "            # agent.update_critic_only(transition_dict)\n",
    "\n",
    "            return_list.append(episode_return)\n",
    "            logger.add(\"pre_train/episode_return\", episode_return, i_episode + 1)\n",
    "            \n",
    "            from Utilities.ModelGradNorm import model_grad_norm\n",
    "            actor_grad_norm = model_grad_norm(agent.actor)\n",
    "            critic_grad_norm = model_grad_norm(agent.critic)\n",
    "            # 梯度监控\n",
    "            logger.add(\"pre_train/actor_grad_norm\", actor_grad_norm, i_episode + 1)\n",
    "            # logger.add(\"pre_train/critic_grad_norm\", critic_grad_norm, i_episode + 1)\n",
    "            # 损失函数监控\n",
    "            logger.add(\"pre_train/actor_loss\", agent.actor_loss, i_episode + 1)\n",
    "            # logger.add(\"pre_train/critic_loss\", agent.critic_loss, i_episode + 1)\n",
    "\n",
    "            \n",
    "            # --- 保存模型（有监督阶段：actor_sup + i_episode，critic 每次覆盖）\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "            # critic overwrite\n",
    "            critic_path = os.path.join(log_dir, \"critic.pt\")\n",
    "            th.save(agent.critic.state_dict(), critic_path)\n",
    "            # actor supervised snapshot\n",
    "            actor_name = f\"actor_sup{i_episode}.pt\"\n",
    "            actor_path = os.path.join(log_dir, actor_name)\n",
    "            th.save(agent.actor.state_dict(), actor_path)\n",
    "            \n",
    "            # tqdm 训练进度显示\n",
    "            if (i_episode + 1) >= 10:\n",
    "                pbar.set_postfix({'episode': '%d' % (i_episode + 1),\n",
    "                                'return': '%.3f' % np.mean(return_list[-10:])})\n",
    "            pbar.update(1)\n",
    "\n",
    "    # 强化学习训练\n",
    "    rl_steps = 0\n",
    "    with tqdm(total=int(num_episodes*(1-pre_train_rate)), desc='Iteration') as pbar:  # 进度条\n",
    "        for i_episode in range(int(num_episodes*(1-pre_train_rate))):  # 每个1/10的训练轮次\n",
    "            episode_return = 0\n",
    "            transition_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': [], 'action_bounds': []}\n",
    "            \n",
    "            init_height = np.random.uniform(4000, 10000)  # 生成一个介于 4000 和 10000 的均匀分布值\n",
    "\n",
    "            birth_state={'position': np.array([-38000.0, init_height, 0.0]),\n",
    "                                'psi': 0\n",
    "                                }\n",
    "            height_req = np.random.uniform(4000, 10000)\n",
    "            env.reset(birth_state=birth_state, height_req=height_req, tacview_show=0, dt_report=dt_decide) # 打乱顺序也行啊\n",
    "            state = env.get_obs()\n",
    "            done = False\n",
    "\n",
    "            actor_grad_list = []\n",
    "            critc_grad_list = []\n",
    "            actor_loss_list = []\n",
    "            critic_loss_list = []\n",
    "            entropy_list = []\n",
    "            ratio_list = []\n",
    "\n",
    "            while not done:  # 每个训练回合\n",
    "                # 1.执行动作得到环境反馈\n",
    "                action = agent.take_action(state, action_bounds=action_bound, explore=True)\n",
    "                rl_steps += 1\n",
    "                total_action = np.array([5000 * action[0], 0, 300]) # 1000 * \n",
    "\n",
    "                next_state, reward, done = env.step(total_action)\n",
    "\n",
    "                transition_dict['states'].append(state)\n",
    "                transition_dict['actions'].append(action)\n",
    "                transition_dict['next_states'].append(next_state)\n",
    "                transition_dict['rewards'].append(reward)\n",
    "                transition_dict['dones'].append(done)\n",
    "                transition_dict['action_bounds'].append(action_bound)\n",
    "                state = next_state\n",
    "                episode_return += reward * env.dt_report # 奖励按秒分析\n",
    "\n",
    "            if env.fail==1:\n",
    "                out_range_count+=1\n",
    "            return_list.append(episode_return)\n",
    "            agent.update(transition_dict)\n",
    "\n",
    "            # --- 保存模型（强化学习阶段：actor_rein + i_episode，critic 每次覆盖）\n",
    "            os.makedirs(log_dir, exist_ok=True)\n",
    "            # critic overwrite\n",
    "            critic_path = os.path.join(log_dir, \"critic.pt\")\n",
    "            th.save(agent.critic.state_dict(), critic_path)\n",
    "            # actor RL snapshot\n",
    "            actor_name = f\"actor_rein{i_episode}.pt\"\n",
    "            actor_path = os.path.join(log_dir, actor_name)\n",
    "            th.save(agent.actor.state_dict(), actor_path)\n",
    "\n",
    "            \n",
    "            # tqdm 训练进度显示\n",
    "            if (i_episode + 1) >= 10:\n",
    "                pbar.set_postfix({'episode': '%d' % (i_episode + 1),\n",
    "                                'return': '%.3f' % np.mean(return_list[-10:])})\n",
    "            pbar.update(1)\n",
    "\n",
    "            # tensorboard 训练进度显示\n",
    "            logger.add(\"train/episode_return\", episode_return, i_episode + 1)\n",
    "\n",
    "            actor_grad_norm = model_grad_norm(agent.actor)\n",
    "            critic_grad_norm = model_grad_norm(agent.critic)\n",
    "            # 梯度监控\n",
    "            logger.add(\"train/actor_grad_norm\", actor_grad_norm, i_episode + 1)\n",
    "            logger.add(\"train/critic_grad_norm\", critic_grad_norm, i_episode + 1)\n",
    "            # 损失函数监控\n",
    "            logger.add(\"train/actor_loss\", agent.actor_loss, i_episode + 1)\n",
    "            logger.add(\"train/critic_loss\", agent.critic_loss, i_episode + 1)\n",
    "            # 强化学习actor特殊项监控\n",
    "            logger.add(\"train/entropy\", agent.entropy_mean, i_episode + 1)\n",
    "            logger.add(\"train/ratio\", agent.ratio_mean, i_episode + 1)\n",
    "\n",
    "    # 在训练结束后，——但仍在 try 范围内——使用最新保存的 actor 权重进行测试\n",
    "    # 优先加载最新 RL 快照，其次 supervised 快照\n",
    "    agent = PPOContinuous(state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                    lmbda, epochs, eps, gamma, device)\n",
    "    if os.path.exists(actor_meta_path):\n",
    "        rein_list = sorted(glob.glob(os.path.join(log_dir, \"actor_rein*.pt\")))\n",
    "        sup_list = sorted(glob.glob(os.path.join(log_dir, \"actor_sup*.pt\")))\n",
    "        latest_actor_path = rein_list[-1] if rein_list else (sup_list[-1] if sup_list else None)\n",
    "        if latest_actor_path:\n",
    "            # 直接加载权重到现有的 agent\n",
    "            sd = th.load(latest_actor_path, map_location=device)\n",
    "            agent.actor.load_state_dict(sd) # , strict=False)  # 忽略缺失的键\n",
    "            print(f\"Loaded actor for test from: {latest_actor_path}\")\n",
    "\n",
    "    # 测试回合（在 try 内，位于 except KeyboardInterrupt 之前）\n",
    "    env.reset(height_req=5e3, dt_report = dt_decide, tacview_show=1)\n",
    "    step = 0\n",
    "    state = env.get_obs()\n",
    "    done = False\n",
    "    while not env.get_done():\n",
    "        action = agent.take_action(state, action_bounds=action_bound, explore=False)\n",
    "\n",
    "        total_action = np.array([5000 * action[0], 0, 300]) # 1000 * \n",
    "\n",
    "        next_state, reward, done = env.step(total_action)\n",
    "        state = next_state\n",
    "        step += 1\n",
    "        env.reder()\n",
    "        time.sleep(0.01)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n检测到 KeyboardInterrupt，正在关闭 logger ...\")\n",
    "finally:\n",
    "    logger.close()\n",
    "    print(f\"日志已保存到：{logger.run_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f2e2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f310d9",
   "metadata": {},
   "source": [
    "这个问题问得非常核心，我仔细说明清楚，不绕弯子。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 回顾：log\\_prob 在 PPO 里为什么重要\n",
    "\n",
    "* PPO 的核心是 **重要性采样比值**：\n",
    "\n",
    "  $$\n",
    "  r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\n",
    "  $$\n",
    "* 其中 $\\pi_\\theta(a_t|s_t)$ 就是策略在给定状态下产生动作 $a_t$ 的概率密度。\n",
    "* 所以我们必须正确计算出 **真实执行动作的概率密度**。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 如果不用 tanh\n",
    "\n",
    "* 假设直接用高斯：\n",
    "\n",
    "  $$\n",
    "  u \\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad a = u\n",
    "  $$\n",
    "* 那么 log\\_prob 就是高斯分布的 log pdf：\n",
    "\n",
    "  $$\n",
    "  \\log \\pi(a|s) = \\log \\mathcal{N}(a|\\mu,\\sigma)\n",
    "  $$\n",
    "\n",
    "没问题。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 用 tanh 压缩动作的情况\n",
    "\n",
    "在 Squashed Gaussian 中：\n",
    "\n",
    "$$\n",
    "u \\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad a = \\tanh(u)\n",
    "$$\n",
    "\n",
    "* 这时环境实际执行的动作是 $a$，而不是 $u$。\n",
    "* 我们需要的概率密度是 **在 $a$-空间下的 pdf**：\n",
    "\n",
    "  $$\n",
    "  \\pi(a|s) = p_U(u) \\cdot \\left|\\det \\frac{\\partial u}{\\partial a}\\right|\n",
    "  $$\n",
    "\n",
    "  这是**变量变换公式**（change of variables formula）。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 变量变换公式具体推导\n",
    "\n",
    "* $a = \\tanh(u)$，所以：\n",
    "\n",
    "  $$\n",
    "  u = \\tanh^{-1}(a) = \\frac{1}{2}\\ln\\frac{1+a}{1-a}\n",
    "  $$\n",
    "* Jacobian：\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial a}{\\partial u} = 1 - \\tanh^2(u) = 1 - a^2\n",
    "  $$\n",
    "* 所以：\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial u}{\\partial a} = \\frac{1}{1-a^2}\n",
    "  $$\n",
    "* 代入变量变换公式：\n",
    "\n",
    "  $$\n",
    "  \\pi(a|s) = \\mathcal{N}(u|\\mu,\\sigma) \\cdot \\left|\\frac{\\partial u}{\\partial a}\\right|\n",
    "  = \\mathcal{N}(\\tanh^{-1}(a)|\\mu,\\sigma) \\cdot \\frac{1}{1-a^2}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 对数概率形式\n",
    "\n",
    "取对数：\n",
    "\n",
    "$$\n",
    "\\log \\pi(a|s) = \\log \\mathcal{N}(\\tanh^{-1}(a)|\\mu,\\sigma) - \\log(1-a^2)\n",
    "$$\n",
    "\n",
    "对于多维动作：Jacobian 是对角矩阵，取行列式等于各维乘积 → 对数就是各维相加：\n",
    "\n",
    "$$\n",
    "\\log \\pi(a|s) = \\log \\mathcal{N}(u|\\mu,\\sigma) - \\sum_i \\log(1-a_i^2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 为什么必须加上这个修正\n",
    "\n",
    "* 如果你直接用 $\\log \\mathcal{N}(a|\\mu,\\sigma)$，那是**在 $u$-空间的概率**，而环境执行的动作是 $a=\\tanh(u)$。\n",
    "* 不加修正 = 把“压缩前的分布”当作“压缩后的分布”，这会导致 **策略概率和真实执行动作概率不一致**。\n",
    "* 结果：importance ratio 出错 → PPO 更新方向错误 → 可能出现梯度爆炸/训练失败。\n",
    "\n",
    "---\n",
    "\n",
    "✅ **总结一句话**：\n",
    "log\\_prob 需要加 Jacobian 修正，是因为 **tanh 改变了随机变量的分布**。\n",
    "真实的 $\\pi(a|s)$ 必须用变量变换公式计算，而这个公式天然带上了 Jacobian 的项 $\\frac{1}{1-a^2}$。\n",
    "\n",
    "---\n",
    "\n",
    "要不要我帮你写一个 **小例子（Python+NumPy）**，对比同一组样本下\n",
    "\n",
    "* “没有修正的 log\\_prob”\n",
    "* “带 Jacobian 修正的 log\\_prob”\n",
    "\n",
    "让你直观看出两者的差别？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f019072",
   "metadata": {},
   "source": [
    "这个问题问得非常核心，我仔细说明清楚，不绕弯子。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 回顾：log\\_prob 在 PPO 里为什么重要\n",
    "\n",
    "* PPO 的核心是 **重要性采样比值**：\n",
    "\n",
    "  $$\n",
    "  r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}\n",
    "  $$\n",
    "* 其中 $\\pi_\\theta(a_t|s_t)$ 就是策略在给定状态下产生动作 $a_t$ 的概率密度。\n",
    "* 所以我们必须正确计算出 **真实执行动作的概率密度**。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 如果不用 tanh\n",
    "\n",
    "* 假设直接用高斯：\n",
    "\n",
    "  $$\n",
    "  u \\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad a = u\n",
    "  $$\n",
    "* 那么 log\\_prob 就是高斯分布的 log pdf：\n",
    "\n",
    "  $$\n",
    "  \\log \\pi(a|s) = \\log \\mathcal{N}(a|\\mu,\\sigma)\n",
    "  $$\n",
    "\n",
    "没问题。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 用 tanh 压缩动作的情况\n",
    "\n",
    "在 Squashed Gaussian 中：\n",
    "\n",
    "$$\n",
    "u \\sim \\mathcal{N}(\\mu, \\sigma^2), \\quad a = \\tanh(u)\n",
    "$$\n",
    "\n",
    "* 这时环境实际执行的动作是 $a$，而不是 $u$。\n",
    "* 我们需要的概率密度是 **在 $a$-空间下的 pdf**：\n",
    "\n",
    "  $$\n",
    "  \\pi(a|s) = p_U(u) \\cdot \\left|\\det \\frac{\\partial u}{\\partial a}\\right|\n",
    "  $$\n",
    "\n",
    "  这是**变量变换公式**（change of variables formula）。\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 变量变换公式具体推导\n",
    "\n",
    "* $a = \\tanh(u)$，所以：\n",
    "\n",
    "  $$\n",
    "  u = \\tanh^{-1}(a) = \\frac{1}{2}\\ln\\frac{1+a}{1-a}\n",
    "  $$\n",
    "* Jacobian：\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial a}{\\partial u} = 1 - \\tanh^2(u) = 1 - a^2\n",
    "  $$\n",
    "* 所以：\n",
    "\n",
    "  $$\n",
    "  \\frac{\\partial u}{\\partial a} = \\frac{1}{1-a^2}\n",
    "  $$\n",
    "* 代入变量变换公式：\n",
    "\n",
    "  $$\n",
    "  \\pi(a|s) = \\mathcal{N}(u|\\mu,\\sigma) \\cdot \\left|\\frac{\\partial u}{\\partial a}\\right|\n",
    "  = \\mathcal{N}(\\tanh^{-1}(a)|\\mu,\\sigma) \\cdot \\frac{1}{1-a^2}\n",
    "  $$\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 对数概率形式\n",
    "\n",
    "取对数：\n",
    "\n",
    "$$\n",
    "\\log \\pi(a|s) = \\log \\mathcal{N}(\\tanh^{-1}(a)|\\mu,\\sigma) - \\log(1-a^2)\n",
    "$$\n",
    "\n",
    "对于多维动作：Jacobian 是对角矩阵，取行列式等于各维乘积 → 对数就是各维相加：\n",
    "\n",
    "$$\n",
    "\\log \\pi(a|s) = \\log \\mathcal{N}(u|\\mu,\\sigma) - \\sum_i \\log(1-a_i^2)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 为什么必须加上这个修正\n",
    "\n",
    "* 如果你直接用 $\\log \\mathcal{N}(a|\\mu,\\sigma)$，那是**在 $u$-空间的概率**，而环境执行的动作是 $a=\\tanh(u)$。\n",
    "* 不加修正 = 把“压缩前的分布”当作“压缩后的分布”，这会导致 **策略概率和真实执行动作概率不一致**。\n",
    "* 结果：importance ratio 出错 → PPO 更新方向错误 → 可能出现梯度爆炸/训练失败。\n",
    "\n",
    "---\n",
    "\n",
    "✅ **总结一句话**：\n",
    "log\\_prob 需要加 Jacobian 修正，是因为 **tanh 改变了随机变量的分布**。\n",
    "真实的 $\\pi(a|s)$ 必须用变量变换公式计算，而这个公式天然带上了 Jacobian 的项 $\\frac{1}{1-a^2}$。\n",
    "\n",
    "---\n",
    "\n",
    "要不要我帮你写一个 **小例子（Python+NumPy）**，对比同一组样本下\n",
    "\n",
    "* “没有修正的 log\\_prob”\n",
    "* “带 Jacobian 修正的 log\\_prob”\n",
    "\n",
    "让你直观看出两者的差别？\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02aad9f",
   "metadata": {},
   "source": [
    "非常好，你已经抓到关键点了。我们把公式和张量维度一一对应：\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 符号约定\n",
    "\n",
    "* $n = \\text{action\\_dim}$ = 动作的维度数。\n",
    "* 输入数据通常是 **一个 batch**：$(\\text{batch\\_size}, n)$。\n",
    "* 我们希望得到的 $\\log \\pi(a|s)$ 是每个样本对应的一个标量 log-prob，用来和 advantage 对齐。\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 单步推导\n",
    "\n",
    "对一个样本（忽略 batch）：\n",
    "\n",
    "$$\n",
    "\\log \\pi(a|s) = \\log \\mathcal{N}(u|\\mu,\\sigma) - \\sum_{i=1}^{n} \\log(1-a_i^2)\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "* $\\log \\mathcal{N}(u|\\mu,\\sigma)$ 是高斯分布在多维下的 log-prob，会返回一个 shape = $(n,)$ 的向量（逐维 log pdf）。\n",
    "* 我们通常会 **沿动作维度求和**，得到一个标量。\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 批量情况\n",
    "\n",
    "假设输入张量形状：\n",
    "\n",
    "* `a`: $(B, n)$\n",
    "* `mu`: $(B, n)$\n",
    "* `std`: $(B, n)$\n",
    "* `u = atanh(a)`: $(B, n)$\n",
    "\n",
    "计算步骤：\n",
    "\n",
    "1. `log_prob_u = Normal(mu, std).log_prob(u)` → shape $(B, n)$\n",
    "2. `jacobian = -torch.log(1 - a**2 + eps)` → shape $(B, n)$\n",
    "3. `(log_prob_u - jacobian).sum(dim=-1, keepdim=True)` → shape $(B, 1)$\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 结果维度\n",
    "\n",
    "$$\n",
    "\\log \\pi(a|s) \\;\\in\\; \\mathbb{R}^{B \\times 1}\n",
    "$$\n",
    "\n",
    "即：**每个样本一行，一个标量 log-prob**。\n",
    "\n",
    "这和 PPO 的计算逻辑对齐：\n",
    "\n",
    "* ratio = $\\exp(\\log \\pi_\\theta - \\log \\pi_{\\theta_{\\text{old}}})$ → shape $(B,1)$\n",
    "* advantage → reshape 成 $(B,1)$\n",
    "* 两者逐元素相乘，再在 batch 上求平均。\n",
    "\n",
    "---\n",
    "\n",
    "✅ **总结一句话**：\n",
    "是的，$n$ 就是 `action_dim`。在代码里，$\\log \\pi(a|s)$ 对一个 batch 会得到 shape = `(batch_size, 1)`，每个样本对应一个 log-prob。\n",
    "\n",
    "---\n",
    "\n",
    "要不要我帮你把这个推导写成一个 **PyTorch demo**，输入 `(batch=4, action_dim=3)`，输出 log\\_prob 的形状和数值，让你直观看到它确实是 `(4,1)`？\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a316d51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最简单的写法：（没有对PPO进行针对性改进）\n",
    "# pi_distribution是网络输出的多元正态分布。\n",
    "logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)\n",
    "logp_pi -= (np.log(1-F.tanh(logp_pi))).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b937cff",
   "metadata": {},
   "source": [
    "简化计算的写法：（openai Spinning up中的SAC）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8377d670",
   "metadata": {},
   "source": [
    "$$\\begin{aligned}&\\log(1-\\tanh^2(u_i))=\\log(\\mathrm{sech}^2(u_i))=2\\log(\\mathrm{sech}(u_i))\\\\&=2\\log(\\frac{2}{\\exp(u_i)+\\exp(-u_i)})=2\\log(2)-\\log(\\exp(u_i)+\\exp(-u_i))\\\\&=2\\log(2)-\\log(\\exp(u_i)+\\exp(-u_i))\\\\&=2\\log(2)-\\log(\\exp(u_i))-\\log(1+\\exp(-2u_i))\\\\&=2\\log(2)-u_{i}-\\mathrm{softplus}(-2u_{i})\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29863a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute logprob from Gaussian, and then apply correction for Tanh squashing.\n",
    "# NOTE: The correction formula is a little bit magic. To get an understanding \n",
    "# of where it comes from, check out the original SAC paper (arXiv 1801.01290) \n",
    "# and look in appendix C. This is a more numerically-stable equivalent to Eq 21.\n",
    "# Try deriving it yourself as a (very difficult) exercise. :)\n",
    "logp_pi = pi_distribution.log_prob(pi_action).sum(axis=-1)\n",
    "logp_pi -= (2*(np.log(2) - pi_action - F.softplus(-2*pi_action))).sum(axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
